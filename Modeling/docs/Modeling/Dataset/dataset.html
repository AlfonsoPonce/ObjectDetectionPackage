<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Modeling.Dataset.dataset API documentation</title>
<meta name="description" content="Module that abstracts three types of dataset format: Pascal VOC, COCO and YOLO …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Modeling.Dataset.dataset</code></h1>
</header>
<section id="section-intro">
<p>Module that abstracts three types of dataset format: Pascal VOC, COCO and YOLO.</p>
<p>Author: Alfonso Ponce Navarro
Date: 05/11/2023</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Module that abstracts three types of dataset format: Pascal VOC, COCO and YOLO.

Author: Alfonso Ponce Navarro
Date: 05/11/2023
&#39;&#39;&#39;
import torch
from PIL import Image, ImageDraw
import numpy as np
from xml.etree import ElementTree as et
import keyboard
from pathlib import Path
from torch.utils.data import Dataset
from albumentations.core.composition import Compose


# the dataset class
class PascalDataset(Dataset):
    &#39;&#39;&#39;
    Class that represents an object detection dataset with Pascal VOC annotation format.
    &#39;&#39;&#39;
    def __init__(
            self, image_list:list, labels_list:list, class_list,
            width:int =0, height:int =0,  transforms: Compose=None
    ):
        &#39;&#39;&#39;
        Instantiate a Pascal VOC dataset.

        :param image_list: list of images
        :param labels_list: list of labels
        :param class_list: list of classes,
        :param width: width resize. If no size if given, image is not resized
        :param height: height resize. If no size if given, image is not resized
        :param transforms: list of albumentations transforms.
        &#39;&#39;&#39;
        self.transforms = transforms
        self.height = height
        self.width = width
        class_list.insert(0, &#39;__bg__&#39;)
        self.class_list = class_list
        self.image_file_types = [&#39;*.jpg&#39;, &#39;*.jpeg&#39;, &#39;*.png&#39;, &#39;*.ppm&#39;]
        self.all_image_paths = []
        self.all_annot_paths = []
        self.images_path = image_list[0].parent
        self.labels_path = labels_list[0].parent
        # get all the image paths in sorted order

        self.all_image_paths.extend(image_list)

        self.all_annot_paths.extend(labels_list)


        self.all_images = [image_path.name for image_path in self.all_image_paths]
        self.all_images = sorted(self.all_images)


    def __getitem__(self, idx: int) -&gt; tuple:
        &#39;&#39;&#39;
        Function to return an object detection instance.

        :param idx: instance index.
        :return: Tuple which contains image and bboxes.
        &#39;&#39;&#39;
        # capture the image name and the full image path
        image_name = self.all_images[idx]
        # print(image_name)
        image_path = self.images_path.joinpath(image_name)

        # read the image
        image_resized = np.array(Image.open(str(image_path))).astype(np.float32)
        if self.width != 0 and self.height != 0:
            image_resized = image_resized.resize((self.width, self.height))

        image_resized /= 255.0

        # capture the corresponding XML file for getting the annotations
        annot_filename = image_name[:-4] + &#39;.xml&#39;
        annot_file_path = self.labels_path.joinpath(annot_filename)

        boxes = []
        labels = []
        tree = et.parse(annot_file_path)
        root = tree.getroot()

        # get the height and width of the image
        image_width = image_resized.shape[1]
        image_height = image_resized.shape[0]

        # box coordinates for xml files are extracted and corrected for image size given
        for member in root.findall(&#39;object&#39;):
            # map the current object name to `classes` list to get...
            # ... the label index and append to `labels` list
            labels.append(self.class_list.index(member.find(&#39;name&#39;).text))

            # xmin = left corner x-coordinates
            xmin = int(member.find(&#39;bndbox&#39;).find(&#39;xmin&#39;).text)
            # xmax = right corner x-coordinates
            xmax = int(member.find(&#39;bndbox&#39;).find(&#39;xmax&#39;).text)
            # ymin = left corner y-coordinates
            ymin = int(member.find(&#39;bndbox&#39;).find(&#39;ymin&#39;).text)
            # ymax = right corner y-coordinates
            ymax = int(member.find(&#39;bndbox&#39;).find(&#39;ymax&#39;).text)

            if self.width != 0 and self.height != 0:
                # resize the bounding boxes according to the...
                # ... desired `width`, `height`
                xmin = (xmin/image_width)*self.width
                xmax = (xmax/image_width)*self.width
                ymin = (ymin/image_height)*self.height
                ymax = (ymax/image_height)*self.height

            #print(f&#39;ANTES {xmax}---{xmin}&#39; )
            if xmax &lt;= xmin: xmax += 0.1
            if ymax &lt;= ymin: ymin -= 0.1
            if ymax &gt; image_height: ymax = image_height - 1
            if xmax &gt; image_width: xmax = image_width - 1
            #if xmin &gt; 1.0: xmin = 1.0
            #if ymin &gt; 1.0: ymin = 1.0
            #print(f&#39;DESPUES {xmax}---{xmin}&#39;)
            boxes.append([xmin, ymin, xmax, ymax])

        # bounding box to tensor
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        # area of the bounding boxes
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        # no crowd instances
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        # labels to tensor
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # prepare the final `target` dictionary
        target = {}
        target[&#34;boxes&#34;] = boxes
        target[&#34;labels&#34;] = labels
        target[&#34;area&#34;] = area
        target[&#34;iscrowd&#34;] = iscrowd
        image_id = torch.tensor([idx])
        target[&#34;image_id&#34;] = image_id
        # apply the image transforms
        if self.transforms:
            sample = self.transforms(image=image_resized,
                                     bboxes=target[&#39;boxes&#39;],
                                     labels=labels)
            image_resized = sample[&#39;image&#39;]
            target[&#39;boxes&#39;] = torch.Tensor(sample[&#39;bboxes&#39;])

        return image_resized, target

    def __len__(self):
        return len(self.all_images)





# execute datasets.py using Python command from Terminal...
# ... to visualize sample images
# USAGE: python datasets.py
if __name__ == &#39;__main__&#39;:
    # sanity check of the Dataset pipeline with sample visualization
    images_list = list(Path(&#39;../../Data/FootballerDetection/raw_data/images&#39;).glob(&#39;*&#39;))
    labels_list = list(Path(&#39;../../Data/FootballerDetection/raw_data/labels&#39;).glob(&#39;*&#39;))
    classes = [&#39;player&#39;, &#39;ball&#39;, &#39;goalkeeper&#39;, &#39;referee&#39;]
    height = 0
    width = 0

    dataset = PascalDataset(
        images_list, labels_list, classes, width, height
    )
    print(f&#34;Number of training images: {len(dataset)}&#34;)


    # function to visualize a single sample
    def visualize_sample(image, target):
        for box_num in range(len(target[&#39;boxes&#39;])):
            box = target[&#39;boxes&#39;][box_num]
            label = classes[target[&#39;labels&#39;][box_num]]
            draw = ImageDraw.Draw(image)
            draw.rectangle((int(box[0]), int(box[1]), int(box[2]), int(box[3])),
                           outline=(0, 255, 0))

            draw.text((int(box[0]), int(box[1] - 5)),label)
        image.show()
        keyboard.wait(&#39;q&#39;)


    NUM_SAMPLES_TO_VISUALIZE = 5
    for i in range(NUM_SAMPLES_TO_VISUALIZE):
        image, target = dataset[i]
        visualize_sample(Image.fromarray(np.uint8(image*255)), target)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Modeling.Dataset.dataset.PascalDataset"><code class="flex name class">
<span>class <span class="ident">PascalDataset</span></span>
<span>(</span><span>image_list: list, labels_list: list, class_list, width: int = 0, height: int = 0, transforms: albumentations.core.composition.Compose = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class that represents an object detection dataset with Pascal VOC annotation format.</p>
<p>Instantiate a Pascal VOC dataset.</p>
<p>:param image_list: list of images
:param labels_list: list of labels
:param class_list: list of classes,
:param width: width resize. If no size if given, image is not resized
:param height: height resize. If no size if given, image is not resized
:param transforms: list of albumentations transforms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PascalDataset(Dataset):
    &#39;&#39;&#39;
    Class that represents an object detection dataset with Pascal VOC annotation format.
    &#39;&#39;&#39;
    def __init__(
            self, image_list:list, labels_list:list, class_list,
            width:int =0, height:int =0,  transforms: Compose=None
    ):
        &#39;&#39;&#39;
        Instantiate a Pascal VOC dataset.

        :param image_list: list of images
        :param labels_list: list of labels
        :param class_list: list of classes,
        :param width: width resize. If no size if given, image is not resized
        :param height: height resize. If no size if given, image is not resized
        :param transforms: list of albumentations transforms.
        &#39;&#39;&#39;
        self.transforms = transforms
        self.height = height
        self.width = width
        class_list.insert(0, &#39;__bg__&#39;)
        self.class_list = class_list
        self.image_file_types = [&#39;*.jpg&#39;, &#39;*.jpeg&#39;, &#39;*.png&#39;, &#39;*.ppm&#39;]
        self.all_image_paths = []
        self.all_annot_paths = []
        self.images_path = image_list[0].parent
        self.labels_path = labels_list[0].parent
        # get all the image paths in sorted order

        self.all_image_paths.extend(image_list)

        self.all_annot_paths.extend(labels_list)


        self.all_images = [image_path.name for image_path in self.all_image_paths]
        self.all_images = sorted(self.all_images)


    def __getitem__(self, idx: int) -&gt; tuple:
        &#39;&#39;&#39;
        Function to return an object detection instance.

        :param idx: instance index.
        :return: Tuple which contains image and bboxes.
        &#39;&#39;&#39;
        # capture the image name and the full image path
        image_name = self.all_images[idx]
        # print(image_name)
        image_path = self.images_path.joinpath(image_name)

        # read the image
        image_resized = np.array(Image.open(str(image_path))).astype(np.float32)
        if self.width != 0 and self.height != 0:
            image_resized = image_resized.resize((self.width, self.height))

        image_resized /= 255.0

        # capture the corresponding XML file for getting the annotations
        annot_filename = image_name[:-4] + &#39;.xml&#39;
        annot_file_path = self.labels_path.joinpath(annot_filename)

        boxes = []
        labels = []
        tree = et.parse(annot_file_path)
        root = tree.getroot()

        # get the height and width of the image
        image_width = image_resized.shape[1]
        image_height = image_resized.shape[0]

        # box coordinates for xml files are extracted and corrected for image size given
        for member in root.findall(&#39;object&#39;):
            # map the current object name to `classes` list to get...
            # ... the label index and append to `labels` list
            labels.append(self.class_list.index(member.find(&#39;name&#39;).text))

            # xmin = left corner x-coordinates
            xmin = int(member.find(&#39;bndbox&#39;).find(&#39;xmin&#39;).text)
            # xmax = right corner x-coordinates
            xmax = int(member.find(&#39;bndbox&#39;).find(&#39;xmax&#39;).text)
            # ymin = left corner y-coordinates
            ymin = int(member.find(&#39;bndbox&#39;).find(&#39;ymin&#39;).text)
            # ymax = right corner y-coordinates
            ymax = int(member.find(&#39;bndbox&#39;).find(&#39;ymax&#39;).text)

            if self.width != 0 and self.height != 0:
                # resize the bounding boxes according to the...
                # ... desired `width`, `height`
                xmin = (xmin/image_width)*self.width
                xmax = (xmax/image_width)*self.width
                ymin = (ymin/image_height)*self.height
                ymax = (ymax/image_height)*self.height

            #print(f&#39;ANTES {xmax}---{xmin}&#39; )
            if xmax &lt;= xmin: xmax += 0.1
            if ymax &lt;= ymin: ymin -= 0.1
            if ymax &gt; image_height: ymax = image_height - 1
            if xmax &gt; image_width: xmax = image_width - 1
            #if xmin &gt; 1.0: xmin = 1.0
            #if ymin &gt; 1.0: ymin = 1.0
            #print(f&#39;DESPUES {xmax}---{xmin}&#39;)
            boxes.append([xmin, ymin, xmax, ymax])

        # bounding box to tensor
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        # area of the bounding boxes
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        # no crowd instances
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        # labels to tensor
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # prepare the final `target` dictionary
        target = {}
        target[&#34;boxes&#34;] = boxes
        target[&#34;labels&#34;] = labels
        target[&#34;area&#34;] = area
        target[&#34;iscrowd&#34;] = iscrowd
        image_id = torch.tensor([idx])
        target[&#34;image_id&#34;] = image_id
        # apply the image transforms
        if self.transforms:
            sample = self.transforms(image=image_resized,
                                     bboxes=target[&#39;boxes&#39;],
                                     labels=labels)
            image_resized = sample[&#39;image&#39;]
            target[&#39;boxes&#39;] = torch.Tensor(sample[&#39;bboxes&#39;])

        return image_resized, target

    def __len__(self):
        return len(self.all_images)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Modeling.Dataset" href="index.html">Modeling.Dataset</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Modeling.Dataset.dataset.PascalDataset" href="#Modeling.Dataset.dataset.PascalDataset">PascalDataset</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>